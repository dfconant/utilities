{
 "metadata": {
  "name": "",
  "signature": "sha256:84f9bf5454a8bfeabc5e0ca386b5f9f49a3e1f9bc52715f55f42745d19063273"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "from ECoGstats import makeD_multi, smooth_formants, smooth_warp_formants, process_kin, normalize_contour_length, SemiPartialCorrelation\n",
      "import seaborn as sns\n",
      "import prettyplotlib as ppl\n",
      "from matplotlib.backends.backend_agg import (FigureCanvasAgg as FigureCanvas)\n",
      "from scipy.signal import savgol_filter\n",
      "import pandas as pd\n",
      "from BDutils import ProgressBar, resample_array, smart_toeplitz, smooth_derivative,seq_labels\n",
      "from ecog_viz import plotWeights, gestural_score, plotERPs, set_style_black\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.linear_model import Ridge\n",
      "import scipy as sp\n",
      "\n",
      "import scipy.stats as stats\n",
      "import matplotlib.gridspec as gridspec\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import VisEcog\n",
      "import imaging\n",
      "import os\n",
      "import matplotlib.image as mpimg\n",
      "import brewer2mpl\n",
      "import re\n",
      "\n",
      "from sklearn import ensemble\n",
      "from sklearn import datasets\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.metrics import mean_squared_error\n",
      "set_style_black()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create dataframes\n",
      "\n",
      "subj = 'EC41'\n",
      "blocks = [19,20,22,27]\n",
      "pth = '/Users/david_conant/Dropbox/Vowels/'+subj + '/'\n",
      "\n",
      "#tokens = ['AAA','IYY','UWW','AEE','AHH','EHH','ERR','IHH','UHH']\n",
      "#tokens = ['HAWED','HEED','WHOOD','HAD','HUD','HEAD','HEARD','HID','HOOD']\n",
      "tokens = ['AAA','IYY','UWW','AEE','AHH','EHH','ERR','IHH','UHH','HAWED','HEED','WHOOD','HAD','HUD','HEAD','HEARD','HID','HOOD']\n",
      "kfeats = ['kFrontTongueX','kFMidTongueX','kMidTongueX','kBMidTongueX','kBackTongueX','kFrontTongueY','kFMidTongueY','kMidTongueY',\n",
      "          'kBMidTongueY','kBackTongueY','kLipOpening','kLipWidth','kJawX','kJawY']\n",
      "efeats = seq_labels('e',256)\n",
      "fs = 100\n",
      "#Load in Dmatrices\n",
      "\n",
      "#Electrodes\n",
      "window = np.array([-1,1])   #TODO: try wider window for more timepoints\n",
      "[E,anat,stop_times,start_times] = makeD_multi(pth,blocks,tokens,align_window = window, dtype='HG')\n",
      "Es = [E[d] for d in tokens]\n",
      "vSMC = np.concatenate((np.where(anat=='preCG')[0],np.where(anat=='postCG')[0]),87)\n",
      "vSMC_logic = np.zeros(256,dtype=bool)\n",
      "vSMC_logic[vSMC] = True\n",
      "Eall = np.concatenate(Es,axis=2)\n",
      "Eall = resample_array(Eall,[Eall.shape[0],np.diff(window)*fs,Eall.shape[2]])\n",
      "\n",
      "#Kinematics\n",
      "#window = np.array([0.25,0.75])\n",
      "[K,anat,stop_times,start_times] = makeD_multi(pth,blocks,tokens,dtype='kin',align_window = window)\n",
      "Ks = [K[d] for d in tokens]\n",
      "stops = [stop_times[d] for d in tokens]\n",
      "starts = [start_times[d] for d in tokens]\n",
      "Ks = process_kin(Ks,stops,starts,window,numTongue=10,wrt_start=True,norm_length=False)\n",
      "Kall = np.concatenate(Ks,axis=2)\n",
      "Kall = resample_array(Kall,[Kall.shape[0],np.diff(window)*fs,Kall.shape[2]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "File found; Loading...\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "File found; Loading..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_coefs = range(1,15)\n",
      "lags = np.linspace(-1,0.25,40)\n",
      "n_cv = 10\n",
      "\n",
      "coefs = np.empty((256,Kall.shape[0],len(lags),len(n_coefs),n_cv))\n",
      "coefs_null = np.empty((256,Kall.shape[0],len(lags),len(n_coefs),n_cv))\n",
      "test_score = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "test_score_null = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "train_score = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "train_score_null = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "mse = np.empty((256,len(lags),len(n_coefs),n_cv))\n",
      "mse_null = np.empty((256,len(lags),len(n_coefs),n_cv))\n",
      "\n",
      "\n",
      "prog = ProgressBar(len(lags))\n",
      "groups = [range(10),range(10,12),range(12,14)]\n",
      "win_size=0.5\n",
      "win_start = 1\n",
      "\n",
      "for ilag, lag in enumerate(lags):\n",
      "    \n",
      "    #Lag by removing data\n",
      "    #t_unlagged = np.round(np.arange(win_start*fs,(win_start+win_size)*fs)).astype(int)\n",
      "    t_unlagged = np.arange(75,125)\n",
      "    t_lagged = np.arange(np.floor((win_start+lag)*fs),np.floor((win_start+win_size)*fs+lag*fs)).astype(int)\n",
      "    Klagged =Kall[:,t_unlagged,:]\n",
      "    Elagged = Eall[:,t_lagged,:]    #behavior-leading lag\n",
      "    trial_blank  = numpy.tile(range(Eall.shape[2]),(1,len(t_unlagged),1))\n",
      "    trial_label = pd.Series(np.reshape(trial_blank,(1,-1),order='F').transpose().squeeze(),name='Trial_Label')\n",
      "    #Reshape across trials\n",
      "    Kinematics = np.reshape(Klagged,(Klagged.shape[0],-1),order='F').transpose()\n",
      "    Kinematics = pd.DataFrame(Kinematics,columns = kfeats)\n",
      "    #Kinematics.fillna(Kinematics.median(),inplace=True)\n",
      "    #Kinematics = pd.Series(dict(enumerate(Eall.transpose())))\n",
      "\n",
      "    HG = np.reshape(Elagged,(Elagged.shape[0],-1),order='F').transpose()\n",
      "    HG = pd.DataFrame(HG,columns = efeats)\n",
      "    vsmcHG = HG.iloc[:,vSMC]\n",
      "    vsmcHG.fillna(vsmcHG.median(),inplace=True)\n",
      "    #vsmcHG.dropna(axis =1,how='any',inplace=True)\n",
      "    #hat,tilde = SemiPartialCorrelation(vsmcHG)\n",
      "    #vsmcHG = tilde\n",
      "    data = pd.concat([vsmcHG,Kinematics,trial_label],axis=1)\n",
      "    data.dropna(axis=1,how='all',inplace=True)   #Drop bad features\n",
      "    data.dropna(axis=0,how='all',inplace=True)   #Drop bad observations\n",
      "    data.dropna(axis=0,how='any',inplace=True)   #Drop bad observations\n",
      "    data.dropna(axis=1,how='any',inplace=True)   #Drop bad features\n",
      "    good_efeats = data.filter(regex=r'^e\\S', axis=1).columns.values\n",
      "    good_kfeats = data.filter(regex=r'^k\\S', axis=1).columns.values\n",
      "    good_elects = [int(e[1:]) for e in good_efeats]\n",
      "    X = data[good_kfeats]\n",
      "    y = data[good_efeats]\n",
      "    trial_label = data['Trial_Label']\n",
      "    \n",
      "    #Split into train and test\n",
      "    num_trials = len(np.unique(trial_label))\n",
      "    \n",
      "    for icv in range(n_cv):\n",
      "        #Cross validate on randomly permuted trials\n",
      "        shuf1 = np.random.permutation(range(num_trials))\n",
      "        train_trials = shuf1[:np.round(num_trials*0.8)]\n",
      "        train_index = trial_label.isin(train_trials)\n",
      "        test_trials = shuf1[np.round(num_trials*0.8):]\n",
      "        test_index = trial_label.isin(test_trials)\n",
      "        X_train , y_train, X_test, y_test = X.loc[train_index,:],y.loc[train_index,:],X.loc[test_index,:],y.loc[test_index,:]\n",
      "        \n",
      "        #create null dataset\n",
      "        null_train = []\n",
      "        null_test = []\n",
      "        [null_train.append(np.where(trial_label.isin([t]))[0]) for t in np.random.permutation(train_trials)]\n",
      "        [null_test.append(np.where(trial_label.isin([t]))[0]) for t in np.random.permutation(test_trials)]\n",
      "        null_train = np.concatenate(np.asarray(null_train),axis=0)\n",
      "        null_test = np.concatenate(np.asarray(null_test),axis=0)\n",
      "        y_train_null,y_test_null = y.iloc[null_train,:],y.iloc[null_test,:]\n",
      "        \n",
      "        \n",
      "        \n",
      "        params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
      "          'learning_rate': 0.01, 'loss': 'ls'}\n",
      "        \n",
      "        for f in range(y_train.shape[1]):\n",
      "            clf = ensemble.GradientBoostingRegressor(**params)\n",
      "            clf.fit(X_train, y_train.iloc[:,f])\n",
      "            mse[good_elects[f],ilag,0,icv] = mean_squared_error(y_test.iloc[:,f], clf.predict(X_test))\n",
      "            for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
      "                test_score[good_elects[f],i,ilag,0,icv] = clf.loss_(y_test.iloc[:,f], y_pred)\n",
      "            feature_importance = clf.feature_importances_\n",
      "            # make importances relative to max importance\n",
      "            coefs[good_elects[f],:,ilag,0,icv] = 100.0 * (feature_importance / feature_importance.max())\n",
      "            print(str(f)),\n",
      "        \n",
      "        '''\n",
      "        for inc,nc in enumerate(n_coefs):\n",
      "            lars = Lars(n_nonzero_coefs = nc)\n",
      "            lars.fit(X_train,y_train)\n",
      "            coefs[good_elects,:,ilag,inc,icv] = lars.coef_\n",
      "            y_hat = lars.predict(X_test)\n",
      "            R2[good_elects,ilag,inc,icv] = [stats.pearsonr(y_test.iloc[:,c],y_hat[:,c])[0] for c in range(y.shape[1])]\n",
      "            \n",
      "            lars_null = Lars(n_nonzero_coefs = nc)\n",
      "            lars_null.fit(X_train,y_train_null)\n",
      "            coefs_null[good_elects,:,ilag,inc,icv] = lars_null.coef_\n",
      "            y_hat_null = lars_null.predict(X_test)\n",
      "            R2_null[good_elects,ilag,inc,icv] = [stats.pearsonr(y_test_null.iloc[:,c],y_hat_null[:,c])[0] for c in range(y.shape[1])]\n",
      "        '''    \n",
      "    prog.animate(ilag+1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "12 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "19 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "22 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "23 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "24 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "26 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "27 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "28 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "29 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "32 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "33 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "34 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "35 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "36 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "38 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "39 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "42 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "43 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "44 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "45 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "46 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "47 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "48 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "49 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "51 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "52 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "53 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "54 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "55 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "56 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "57 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "58 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "59 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "60 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "61"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-27-ccb59ce6077c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mmse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgood_elects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0milag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0micv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/david_conant/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m--> 980\u001b[0;31m                                     begin_at_stage, monitor)\n\u001b[0m\u001b[1;32m    981\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/david_conant/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1039\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                                      random_state)\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/david_conant/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, criterion, splitter, random_state)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 766\u001b[0;31m                      check_input=False)\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/david_conant/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                            max_leaf_nodes)\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_score.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(256, 40, 14, 10)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "86"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import ensemble\n",
      "from sklearn import datasets\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "###############################################################################\n",
      "# Load data\n",
      "boston = datasets.load_boston()\n",
      "X, y = shuffle(boston.data, boston.target, random_state=13)\n",
      "X = X.astype(np.float32)\n",
      "offset = int(X.shape[0] * 0.9)\n",
      "X_train, y_train = X[:offset], y[:offset]\n",
      "X_test, y_test = X[offset:], y[offset:]\n",
      "\n",
      "###############################################################################\n",
      "# Fit regression model\n",
      "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
      "          'learning_rate': 0.01, 'loss': 'ls'}\n",
      "clf = ensemble.GradientBoostingRegressor(**params)\n",
      "\n",
      "clf.fit(X_train, y_train)\n",
      "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
      "print(\"MSE: %.4f\" % mse)\n",
      "\n",
      "###############################################################################\n",
      "# Plot training deviance\n",
      "\n",
      "# compute test set deviance\n",
      "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
      "\n",
      "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
      "    test_score[i] = clf.loss_(y_test, y_pred)\n",
      "\n",
      "plt.figure(figsize=(12, 6))\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.title('Deviance')\n",
      "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
      "         label='Training Set Deviance')\n",
      "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
      "         label='Test Set Deviance')\n",
      "plt.legend(loc='upper right')\n",
      "plt.xlabel('Boosting Iterations')\n",
      "plt.ylabel('Deviance')\n",
      "\n",
      "###############################################################################\n",
      "# Plot feature importance\n",
      "feature_importance = clf.feature_importances_\n",
      "# make importances relative to max importance\n",
      "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
      "sorted_idx = np.argsort(feature_importance)\n",
      "pos = np.arange(sorted_idx.shape[0]) + .5\n",
      "plt.subplot(1, 2, 2)\n",
      "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
      "plt.yticks(pos, boston.feature_names[sorted_idx])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.title('Variable Importance')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}