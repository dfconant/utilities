{
 "metadata": {
  "name": "",
  "signature": "sha256:34b2776968c065008c3928585934f37af0811541269ccc578614473c8b23e7b1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "from ECoGstats import makeD_multi, smooth_formants, smooth_warp_formants, process_kin, normalize_contour_length, SemiPartialCorrelation\n",
      "import seaborn as sns\n",
      "import prettyplotlib as ppl\n",
      "from matplotlib.backends.backend_agg import (FigureCanvasAgg as FigureCanvas)\n",
      "from scipy.signal import savgol_filter\n",
      "import pandas as pd\n",
      "from BDutils import ProgressBar, resample_array, smart_toeplitz, smooth_derivative,seq_labels\n",
      "from ecog_viz import plotWeights, gestural_score, plotERPs, set_style_black\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.linear_model import Ridge\n",
      "import scipy as sp\n",
      "\n",
      "import scipy.stats as stats\n",
      "import matplotlib.gridspec as gridspec\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import VisEcog\n",
      "import imaging\n",
      "import os\n",
      "import matplotlib.image as mpimg\n",
      "import brewer2mpl\n",
      "import re\n",
      "\n",
      "from sklearn import ensemble\n",
      "from sklearn import datasets\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.metrics import mean_squared_error\n",
      "set_style_black()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFVCAYAAADVDycqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxBkAmhFnAGhGnAGgmf8FUeh2o6HjWf0AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x112cc7850>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create dataframes\n",
      "\n",
      "subj = 'EC41'\n",
      "blocks = [19,20,22,27]\n",
      "pth = '/Users/david_conant/Dropbox/Vowels/'+subj + '/'\n",
      "\n",
      "#tokens = ['AAA','IYY','UWW','AEE','AHH','EHH','ERR','IHH','UHH']\n",
      "#tokens = ['HAWED','HEED','WHOOD','HAD','HUD','HEAD','HEARD','HID','HOOD']\n",
      "tokens = ['AAA','IYY','UWW','AEE','AHH','EHH','ERR','IHH','UHH','HAWED','HEED','WHOOD','HAD','HUD','HEAD','HEARD','HID','HOOD']\n",
      "kfeats = ['kFrontTongueX','kFMidTongueX','kMidTongueX','kBMidTongueX','kBackTongueX','kFrontTongueY','kFMidTongueY','kMidTongueY',\n",
      "          'kBMidTongueY','kBackTongueY','kLipOpening','kLipWidth','kJawX','kJawY']\n",
      "efeats = seq_labels('e',256)\n",
      "fs = 100\n",
      "#Load in Dmatrices\n",
      "\n",
      "#Electrodes\n",
      "window = np.array([-1,1])   #TODO: try wider window for more timepoints\n",
      "[E,anat,stop_times,start_times] = makeD_multi(pth,blocks,tokens,align_window = window, dtype='HG')\n",
      "Es = [E[d] for d in tokens]\n",
      "vSMC = np.concatenate((np.where(anat=='preCG')[0],np.where(anat=='postCG')[0]),87)\n",
      "vSMC_logic = np.zeros(256,dtype=bool)\n",
      "vSMC_logic[vSMC] = True\n",
      "Eall = np.concatenate(Es,axis=2)\n",
      "Eall = resample_array(Eall,[Eall.shape[0],np.diff(window)*fs,Eall.shape[2]])\n",
      "\n",
      "#Kinematics\n",
      "#window = np.array([0.25,0.75])\n",
      "[K,anat,stop_times,start_times] = makeD_multi(pth,blocks,tokens,dtype='kin',align_window = window)\n",
      "Ks = [K[d] for d in tokens]\n",
      "stops = [stop_times[d] for d in tokens]\n",
      "starts = [start_times[d] for d in tokens]\n",
      "Ks = process_kin(Ks,stops,starts,window,numTongue=10,wrt_start=True,norm_length=False)\n",
      "Kall = np.concatenate(Ks,axis=2)\n",
      "Kall = resample_array(Kall,[Kall.shape[0],np.diff(window)*fs,Kall.shape[2]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "File found; Loading...\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "File found; Loading..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_coefs = range(1,15)\n",
      "lags = np.linspace(-1,0.25,40)\n",
      "n_cv = 10\n",
      "\n",
      "coefs = np.empty((256,Kall.shape[0],len(lags),len(n_coefs),n_cv))\n",
      "coefs_null = np.empty((256,Kall.shape[0],len(lags),len(n_coefs),n_cv))\n",
      "test_score = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "test_score_null = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "train_score = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "train_score_null = np.empty((256,500,len(lags),len(n_coefs),n_cv))\n",
      "mse = np.empty((256,len(lags),len(n_coefs),n_cv))\n",
      "mse_null = np.empty((256,len(lags),len(n_coefs),n_cv))\n",
      "\n",
      "\n",
      "prog = ProgressBar(len(lags))\n",
      "groups = [range(10),range(10,12),range(12,14)]\n",
      "win_size=0.5\n",
      "win_start = 1\n",
      "\n",
      "for ilag, lag in enumerate(lags):\n",
      "    \n",
      "    #Lag by removing data\n",
      "    #t_unlagged = np.round(np.arange(win_start*fs,(win_start+win_size)*fs)).astype(int)\n",
      "    t_unlagged = np.arange(75,125)\n",
      "    t_lagged = np.arange(np.floor((win_start+lag)*fs),np.floor((win_start+win_size)*fs+lag*fs)).astype(int)\n",
      "    Klagged =Kall[:,t_unlagged,:]\n",
      "    Elagged = Eall[:,t_lagged,:]    #behavior-leading lag\n",
      "    trial_blank  = numpy.tile(range(Eall.shape[2]),(1,len(t_unlagged),1))\n",
      "    trial_label = pd.Series(np.reshape(trial_blank,(1,-1),order='F').transpose().squeeze(),name='Trial_Label')\n",
      "    #Reshape across trials\n",
      "    Kinematics = np.reshape(Klagged,(Klagged.shape[0],-1),order='F').transpose()\n",
      "    Kinematics = pd.DataFrame(Kinematics,columns = kfeats)\n",
      "    #Kinematics.fillna(Kinematics.median(),inplace=True)\n",
      "    #Kinematics = pd.Series(dict(enumerate(Eall.transpose())))\n",
      "\n",
      "    HG = np.reshape(Elagged,(Elagged.shape[0],-1),order='F').transpose()\n",
      "    HG = pd.DataFrame(HG,columns = efeats)\n",
      "    vsmcHG = HG.iloc[:,vSMC]\n",
      "    vsmcHG.fillna(vsmcHG.median(),inplace=True)\n",
      "    #vsmcHG.dropna(axis =1,how='any',inplace=True)\n",
      "    #hat,tilde = SemiPartialCorrelation(vsmcHG)\n",
      "    #vsmcHG = tilde\n",
      "    data = pd.concat([vsmcHG,Kinematics,trial_label],axis=1)\n",
      "    data.dropna(axis=1,how='all',inplace=True)   #Drop bad features\n",
      "    data.dropna(axis=0,how='all',inplace=True)   #Drop bad observations\n",
      "    data.dropna(axis=0,how='any',inplace=True)   #Drop bad observations\n",
      "    data.dropna(axis=1,how='any',inplace=True)   #Drop bad features\n",
      "    good_efeats = data.filter(regex=r'^e\\S', axis=1).columns.values\n",
      "    good_kfeats = data.filter(regex=r'^k\\S', axis=1).columns.values\n",
      "    good_elects = [int(e[1:]) for e in good_efeats]\n",
      "    X = data[good_kfeats]\n",
      "    y = data[good_efeats]\n",
      "    trial_label = data['Trial_Label']\n",
      "    \n",
      "    #Split into train and test\n",
      "    num_trials = len(np.unique(trial_label))\n",
      "    \n",
      "    for icv in range(n_cv):\n",
      "        #Cross validate on randomly permuted trials\n",
      "        shuf1 = np.random.permutation(range(num_trials))\n",
      "        train_trials = shuf1[:np.round(num_trials*0.8)]\n",
      "        train_index = trial_label.isin(train_trials)\n",
      "        test_trials = shuf1[np.round(num_trials*0.8):]\n",
      "        test_index = trial_label.isin(test_trials)\n",
      "        X_train , y_train, X_test, y_test = X.loc[train_index,:],y.loc[train_index,:],X.loc[test_index,:],y.loc[test_index,:]\n",
      "        \n",
      "        #create null dataset\n",
      "        null_train = []\n",
      "        null_test = []\n",
      "        [null_train.append(np.where(trial_label.isin([t]))[0]) for t in np.random.permutation(train_trials)]\n",
      "        [null_test.append(np.where(trial_label.isin([t]))[0]) for t in np.random.permutation(test_trials)]\n",
      "        null_train = np.concatenate(np.asarray(null_train),axis=0)\n",
      "        null_test = np.concatenate(np.asarray(null_test),axis=0)\n",
      "        y_train_null,y_test_null = y.iloc[null_train,:],y.iloc[null_test,:]\n",
      "        \n",
      "        \n",
      "        \n",
      "        params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
      "          'learning_rate': 0.01, 'loss': 'ls'}\n",
      "        \n",
      "        for f in range(y_train.shape[1]):\n",
      "            clf = ensemble.GradientBoostingRegressor(**params)\n",
      "            clf.fit(X_train, y_train.iloc[:,f])\n",
      "            mse[good_elects[f],ilag,0,icv] = mean_squared_error(y_test.iloc[:,f], clf.predict(X_test))\n",
      "            for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
      "                test_score[good_elects[f],i,ilag,0,icv] = clf.loss_(y_test.iloc[:,f], y_pred)\n",
      "            feature_importance = clf.feature_importances_\n",
      "            # make importances relative to max importance\n",
      "            coefs[good_elects[f],:,ilag,0,icv] = 100.0 * (feature_importance / feature_importance.max())\n",
      "            print(str(f)),\n",
      "        \n",
      "        '''\n",
      "        for inc,nc in enumerate(n_coefs):\n",
      "            lars = Lars(n_nonzero_coefs = nc)\n",
      "            lars.fit(X_train,y_train)\n",
      "            coefs[good_elects,:,ilag,inc,icv] = lars.coef_\n",
      "            y_hat = lars.predict(X_test)\n",
      "            R2[good_elects,ilag,inc,icv] = [stats.pearsonr(y_test.iloc[:,c],y_hat[:,c])[0] for c in range(y.shape[1])]\n",
      "            \n",
      "            lars_null = Lars(n_nonzero_coefs = nc)\n",
      "            lars_null.fit(X_train,y_train_null)\n",
      "            coefs_null[good_elects,:,ilag,inc,icv] = lars_null.coef_\n",
      "            y_hat_null = lars_null.predict(X_test)\n",
      "            R2_null[good_elects,ilag,inc,icv] = [stats.pearsonr(y_test_null.iloc[:,c],y_hat_null[:,c])[0] for c in range(y.shape[1])]\n",
      "        '''    \n",
      "    prog.animate(ilag+1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 "
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_score.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(256, 40, 14, 10)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "86"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import ensemble\n",
      "from sklearn import datasets\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "###############################################################################\n",
      "# Load data\n",
      "boston = datasets.load_boston()\n",
      "X, y = shuffle(boston.data, boston.target, random_state=13)\n",
      "X = X.astype(np.float32)\n",
      "offset = int(X.shape[0] * 0.9)\n",
      "X_train, y_train = X[:offset], y[:offset]\n",
      "X_test, y_test = X[offset:], y[offset:]\n",
      "\n",
      "###############################################################################\n",
      "# Fit regression model\n",
      "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
      "          'learning_rate': 0.01, 'loss': 'ls'}\n",
      "clf = ensemble.GradientBoostingRegressor(**params)\n",
      "\n",
      "clf.fit(X_train, y_train)\n",
      "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
      "print(\"MSE: %.4f\" % mse)\n",
      "\n",
      "###############################################################################\n",
      "# Plot training deviance\n",
      "\n",
      "# compute test set deviance\n",
      "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
      "\n",
      "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
      "    test_score[i] = clf.loss_(y_test, y_pred)\n",
      "\n",
      "plt.figure(figsize=(12, 6))\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.title('Deviance')\n",
      "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
      "         label='Training Set Deviance')\n",
      "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
      "         label='Test Set Deviance')\n",
      "plt.legend(loc='upper right')\n",
      "plt.xlabel('Boosting Iterations')\n",
      "plt.ylabel('Deviance')\n",
      "\n",
      "###############################################################################\n",
      "# Plot feature importance\n",
      "feature_importance = clf.feature_importances_\n",
      "# make importances relative to max importance\n",
      "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
      "sorted_idx = np.argsort(feature_importance)\n",
      "pos = np.arange(sorted_idx.shape[0]) + .5\n",
      "plt.subplot(1, 2, 2)\n",
      "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
      "plt.yticks(pos, boston.feature_names[sorted_idx])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.title('Variable Importance')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}